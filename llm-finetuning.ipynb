{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Large Language Model (LLM) Finetuning\n",
    "\n",
    "This notebook contains code to evaluate an LLM on a subset of the [SQuAD dataset](https://huggingface.co/datasets/rajpurkar/squad) (Stanford Question Answering Dataset), fine-tune it on it and reevaluate to check model's performance. Along the way, we'll stop and explain several of the concepts involved across similar tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "\n",
    "The problem at hand is a subfield of Natural Language Processing (NLP) called Question Answering (QA). The goal is to, asking an LLM a question given some context, receive an appropriate answer included in the beforementioned context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xavia\\Projects\\llm-finetuning\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import DistilBertForQuestionAnswering, DistilBertTokenizerFast, Trainer, TrainingArguments, pipeline\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model\n",
    "\n",
    "Nowadays, LLMs are versatile enough to address this and many other problems through prompt engineering. Within this framework, engineers tweak their prompts in order to get the best possible results to their problems. However, generalist LLMs can become unfeasible to use depending on computational, budget and response time constraints. This is why, depending on the problem at hand, a more direct approach might be better fitting.  \n",
    "One personally recommended course of action is to first check the available models that aims to solve the problem of interest. The HuggingFace (HF) Hub is a well known initiative where to check many resources, including models and datasets. This way, it's easy to check the best models for a particular task.  \n",
    "Besides model comparison regarding purely evaluation metrics, other very important aspect of LLM deployment is its size. Many of them are traditionally large enough to prove themselves challenging to host. A useful tool might be the [Can you run it? LLM version](https://huggingface.co/spaces/Vokturz/can-it-run-llm) from HF. It allows the user to select a model, hardware, and the web will display if it's feasible or not to run it on 1 or more GPUs depending on quantization, training adequacy, etc.  \n",
    "For this particular project, which is meant to showcase how to perform fine-tuning and evaluations in a normal setup rather than finding the best possible solution, we'll start from [DistilBERT base model](https://huggingface.co/distilbert/distilbert-base-uncased) rather than already fine-tuned ones in the desired Dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"distilbert-base-uncased\"\n",
    "model = DistilBertForQuestionAnswering.from_pretrained(model_name)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Dataset\n",
    "\n",
    "The Dataset of interest is the Stanford Question Answering Dataset ([SQuAD](https://huggingface.co/datasets/rajpurkar/squad)). It comprehends a set of segments of text from Wikipedia (context) alongside questions and answers that can be found in the given context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "squad = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Preprocessing\n",
    "\n",
    "Usually, Deep Learning (FL) applications require some preprocessing to their inputs. In NLP, this may involve some text cleaning, tokenization (technique that depends on the model of choice), truncation handling, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    answers = examples[\"answers\"]\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offset in enumerate(offset_mapping):\n",
    "        answer = answers[i]\n",
    "        start_char = answer[\"answer_start\"][0]\n",
    "        end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "        # Find the start and end of the context\n",
    "        idx = 0\n",
    "        while sequence_ids[idx] != 1:\n",
    "            idx += 1\n",
    "        context_start = idx\n",
    "        while sequence_ids[idx] == 1:\n",
    "            idx += 1\n",
    "        context_end = idx - 1\n",
    "\n",
    "        # If the answer is not fully inside the context, label it (0, 0)\n",
    "        if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "            start_positions.append(0)\n",
    "            end_positions.append(0)\n",
    "        else:\n",
    "            # Otherwise it's the start and end token positions\n",
    "            idx = context_start\n",
    "            while idx <= context_end and offset[idx][0] <= start_char:\n",
    "                idx += 1\n",
    "            start_positions.append(idx - 1)\n",
    "\n",
    "            idx = context_end\n",
    "            while idx >= context_start and offset[idx][1] >= end_char:\n",
    "                idx -= 1\n",
    "            end_positions.append(idx + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_squad = squad.map(preprocess_function, batched=True, \n",
    "                            remove_columns=squad[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning\n",
    "\n",
    "Training an LLM from scratch can be very slow and costly. Instead, one common practice in DL is to start from an already pre-trained model and start training from there (what we call fine-tuning). The code below shows the training configuration through HF: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\xavia\\Projects\\llm-finetuning\\venv\\Lib\\site-packages\\transformers\\training_args.py:1559: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=500,\n",
    "    report_to=\"tensorboard\", \n",
    "    save_steps=500,\n",
    "    save_total_limit=3,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right after it, it's possible to evaluate the initial model against the SQuAD's *validation* subset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661/661 [00:28<00:00, 22.93it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.8978590965271,\n",
       " 'eval_model_preparation_time': 0.001,\n",
       " 'eval_runtime': 29.405,\n",
       " 'eval_samples_per_second': 359.462,\n",
       " 'eval_steps_per_second': 22.479}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After setting a baseline, let's start the QA model's fine-tuning.  \n",
    "**Note:** This step can take a significant amount of time depending on hardware specifications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This training will create some output logs that can be read with TensorBoard. You can start TensorBoard with the following command:\n",
    "\n",
    "```bash\n",
    "tensorboard --logdir logs/\n",
    "```\n",
    "\n",
    "TensorBoard provides useful information in a visual way. For example, monitoring the train and eval losses while training can give information about the current's training state. For instance, if both losses are high and do not decrease over time, the model may be underfitting. On the other hand, if the train loss decreases but the eval one starts to increase, it might be overfitting. Depending on the scenario, the engineer might choose to look for more powerful architectures, more broad and representative data, or start with hyperparameter (HP) tuning. The most common one to tweak is the Learning Rate (LR): one too big might yield to quick improvements at the risk of reaching a loss plateau. On the other hand, a smaller one might make the training too slow. It's recommended to play with HP for the optimizers (e.g. Adam) or different strategies to get the best possible results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a model has already been fine-tuned, specify the checkpoint of your choice below to load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_checkpoint = \"./results/checkpoint-10000\"\n",
    "\n",
    "finetuned_model = DistilBertForQuestionAnswering.from_pretrained(best_checkpoint)\n",
    "tokenizer = DistilBertTokenizerFast.from_pretrained(\"distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation comparison\n",
    "\n",
    "After having available both models, the pre-trained and the fine-tuned ones, let's make a more thorough evaluation comparison:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661/661 [00:29<00:00, 22.71it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.956923484802246,\n",
       " 'eval_model_preparation_time': 0.001,\n",
       " 'eval_runtime': 29.3675,\n",
       " 'eval_samples_per_second': 359.922,\n",
       " 'eval_steps_per_second': 22.508}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model evaluation\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 661/661 [00:29<00:00, 22.56it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.1106374263763428,\n",
       " 'eval_model_preparation_time': 0.001,\n",
       " 'eval_runtime': 29.3136,\n",
       " 'eval_samples_per_second': 360.584,\n",
       " 'eval_steps_per_second': 22.549}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuned model evaluation\n",
    "trainer = Trainer(\n",
    "    model=finetuned_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_squad[\"train\"],\n",
    "    eval_dataset=tokenized_squad[\"validation\"],\n",
    ")\n",
    "\n",
    "trainer.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The evaluation loss difference (lower is better) already shows that the fine-tuned model is better than the baseline.  \n",
    "Furthermore, it's possible to make a more thorough analysis by generating the model's responses and performing an Error Analysis (EA). For that purpose, let's use the HF's *question answering pipeline*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForQuestionAnswering were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    }
   ],
   "source": [
    "qa_pipeline = pipeline(\"question-answering\", \n",
    "                       model=\"distilbert-base-uncased\", \n",
    "                       tokenizer=tokenizer, \n",
    "                       batch_size=64)\n",
    "\n",
    "finetuned_qa_pipeline = pipeline(\"question-answering\", \n",
    "                                 model=finetuned_model, \n",
    "                                 tokenizer=tokenizer,\n",
    "                                 batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's reuse the SQuAD's validation dataset to store the predicted answers and scores for both models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(example):\n",
    "    baseline_prediction = qa_pipeline(question=example[\"question\"], context=example[\"context\"])\n",
    "    finetuned_prediction = finetuned_qa_pipeline(question=example[\"question\"], context=example[\"context\"])\n",
    "    return {\n",
    "        \"baseline_prediction\": baseline_prediction[\"answer\"],\n",
    "        \"baseline_score\": baseline_prediction[\"score\"],\n",
    "        \"finetuned_prediction\": finetuned_prediction[\"answer\"],\n",
    "        \"finetuned_score\": finetuned_prediction[\"score\"],\n",
    "        \"ground_truth\": example[\"answers\"][\"text\"][0]\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10570/10570 [10:34<00:00, 16.66 examples/s]\n"
     ]
    }
   ],
   "source": [
    "predictions = squad[\"validation\"].map(get_prediction, batch_size=64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this data at hand, let's use the *HF's evaluate* library to check more evaluation metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SQuAD metric is expecting a list of dictionaries containing:\n",
    "* **id:** ID of the sample.\n",
    "* **answers:** Predicted answer or list of ground truth answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_predicted_answers = [{\"id\": pred[\"id\"],\n",
    "                               \"prediction_text\": pred[\"baseline_prediction\"]}\n",
    "                               for pred in predictions]\n",
    "finetuned_predicted_answers = [{\"id\": pred[\"id\"],\n",
    "                                \"prediction_text\": pred[\"finetuned_prediction\"]}\n",
    "                                for pred in predictions]\n",
    "theoretical_answers = [\n",
    "    {\"id\": ex[\"id\"], \"answers\": ex[\"answers\"]} for ex in predictions\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.8609271523178808, 'f1': 7.889010128219963}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Baseline model's evaluation metrics\n",
    "metric.compute(predictions=baseline_predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 76.59413434247871, 'f1': 84.8074232750765}"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fine-tuned model's evaluation metrics\n",
    "metric.compute(predictions=finetuned_predicted_answers, references=theoretical_answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, it's even more clear that the fine-tuned model is way superior than the baseline. The exact matches indicates the cases where the predictions matched precisely the ground truth answer.  \n",
    "On the other hand, the F1-score is the harmonic mean of the Precision and the Recall, which can be calculated with:\n",
    "* True Positive: Number of shared tokens between the prediction and the correct answer.\n",
    "* False Positive: Number of tokens in the predicted sequence, excluding the shared tokens.\n",
    "* False Negative: Number of tokens in the correct answer, excluding the shared tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, by converting the *HF dataset* into pandas, we gain more control about the data and how to analyze it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = predictions.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>baseline_score</th>\n",
       "      <th>finetuned_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10570.000000</td>\n",
       "      <td>10570.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.572211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.000272</td>\n",
       "      <td>0.293062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000023</td>\n",
       "      <td>0.006408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000090</td>\n",
       "      <td>0.325123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000146</td>\n",
       "      <td>0.578012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000226</td>\n",
       "      <td>0.849143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.003764</td>\n",
       "      <td>0.999911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       baseline_score  finetuned_score\n",
       "count    10570.000000     10570.000000\n",
       "mean         0.000209         0.572211\n",
       "std          0.000272         0.293062\n",
       "min          0.000023         0.006408\n",
       "25%          0.000090         0.325123\n",
       "50%          0.000146         0.578012\n",
       "75%          0.000226         0.849143\n",
       "max          0.003764         0.999911"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>context</th>\n",
       "      <th>baseline_prediction</th>\n",
       "      <th>finetuned_prediction</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>baseline_score</th>\n",
       "      <th>finetuned_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What day was the game played on?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>February 7, 2016</td>\n",
       "      <td>February 7, 2016</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.862131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>Carolina Panthers</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.766022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What does AFC stand for?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>American Football Conference</td>\n",
       "      <td>American Football Conference</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>0.601211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>Levi's Stadium</td>\n",
       "      <td>Santa Clara, California</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.500625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.355355</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>Denver Broncos</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.240282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>(NFL) for the 2015 season. The American Footba...</td>\n",
       "      <td>gold</td>\n",
       "      <td>gold</td>\n",
       "      <td>0.000121</td>\n",
       "      <td>0.227247</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the AFC short for?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>American Football Conference</td>\n",
       "      <td>American Football Conference</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.186964</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What was the theme of Super Bowl 50?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>golden anniversary\"</td>\n",
       "      <td>\"golden anniversary\"</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.043139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>What was the theme of Super Bowl 50?</td>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>earn their third Super Bowl title.</td>\n",
       "      <td>golden anniversary\"</td>\n",
       "      <td>\"golden anniversary\"</td>\n",
       "      <td>0.000123</td>\n",
       "      <td>0.043139</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            question  \\\n",
       "6                   What day was the game played on?   \n",
       "1  Which NFL team represented the NFC at Super Bo...   \n",
       "9                           What does AFC stand for?   \n",
       "2                Where did Super Bowl 50 take place?   \n",
       "0  Which NFL team represented the AFC at Super Bo...   \n",
       "3                  Which NFL team won Super Bowl 50?   \n",
       "4  What color was used to emphasize the 50th anni...   \n",
       "7                         What is the AFC short for?   \n",
       "5               What was the theme of Super Bowl 50?   \n",
       "8               What was the theme of Super Bowl 50?   \n",
       "\n",
       "                                             context  \\\n",
       "6  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "9  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "3  Super Bowl 50 was an American football game to...   \n",
       "4  Super Bowl 50 was an American football game to...   \n",
       "7  Super Bowl 50 was an American football game to...   \n",
       "5  Super Bowl 50 was an American football game to...   \n",
       "8  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                 baseline_prediction  \\\n",
       "6                 earn their third Super Bowl title.   \n",
       "1                 earn their third Super Bowl title.   \n",
       "9                 earn their third Super Bowl title.   \n",
       "2                 earn their third Super Bowl title.   \n",
       "0                 earn their third Super Bowl title.   \n",
       "3                 earn their third Super Bowl title.   \n",
       "4  (NFL) for the 2015 season. The American Footba...   \n",
       "7                 earn their third Super Bowl title.   \n",
       "5                 earn their third Super Bowl title.   \n",
       "8                 earn their third Super Bowl title.   \n",
       "\n",
       "           finetuned_prediction                  ground_truth  baseline_score  \\\n",
       "6              February 7, 2016              February 7, 2016        0.000123   \n",
       "1             Carolina Panthers             Carolina Panthers        0.000121   \n",
       "9  American Football Conference  American Football Conference        0.000122   \n",
       "2                Levi's Stadium       Santa Clara, California        0.000123   \n",
       "0                Denver Broncos                Denver Broncos        0.000121   \n",
       "3                Denver Broncos                Denver Broncos        0.000123   \n",
       "4                          gold                          gold        0.000121   \n",
       "7  American Football Conference  American Football Conference        0.000123   \n",
       "5           golden anniversary\"          \"golden anniversary\"        0.000123   \n",
       "8           golden anniversary\"          \"golden anniversary\"        0.000123   \n",
       "\n",
       "   finetuned_score  \n",
       "6         0.862131  \n",
       "1         0.766022  \n",
       "9         0.601211  \n",
       "2         0.500625  \n",
       "0         0.355355  \n",
       "3         0.240282  \n",
       "4         0.227247  \n",
       "7         0.186964  \n",
       "5         0.043139  \n",
       "8         0.043139  "
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10).sort_values(\"finetuned_score\", ascending=False)[[\"question\", \"context\", \"baseline_prediction\", \"finetuned_prediction\", \"ground_truth\", \"baseline_score\", \"finetuned_score\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "The notebook covered this points:\n",
    "* General Question Answering considerations and problem definition.\n",
    "* Model selection: DistilBert as a relatively lightweight model for fine-tuning convenience.\n",
    "* Model fine-tuning: Use of HF Trainer to fine-tune the model, plus HP tuning, TensorBoard logging and monitoring, etc.\n",
    "* Evaluation comparison: Use of HF Evaluate to compare results for both baseline and fine-tuned models, showing that the fine-tuned one clearly outperforms the initial one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "* https://huggingface.co/docs/transformers/tasks/question_answering\n",
    "* https://huggingface.co/learn/nlp-course/chapter7/7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
